# -*- coding: utf-8 -*-

import json
import os
import time
from datetime import datetime
from dotenv import load_dotenv
import google.generativeai as genai
from tqdm import tqdm
import re
from kiwipiepy import Kiwi

class TrainingDataPreprocessor:
    def __init__(self):
        # .env íŒŒì¼ ë¡œë“œ
        load_dotenv()
        
        # Gemini API ì„¤ì •
        api_key = os.getenv('Gemini_API_Key')
        if not api_key:
            raise ValueError("Gemini_API_Keyê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel('gemini-2.0-flash-exp')
        
        # í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸° (ìˆœìˆ˜ Python ë²„ì „)
        self.kiwi = Kiwi()
        
        # í‰ê°€ ê¸°ì¤€ í…œí”Œë¦¿ (ê¸°ì¡´ê³¼ ë™ì¼)
        self.evaluation_templates = {
            'cdi': {
                'academic_achievement': {
                    'positive': ['ì–´ë µì§€ ì•Šë‹¤', 'ì‰½ë‹¤', 'ì˜í•˜ë‹¤', 'ê´œì°®ë‹¤', 'ë¬¸ì œì—†ë‹¤', 'ì¢‹ë‹¤', 'ì¬ë¯¸ìˆë‹¤', 'ì„±ê³µí•˜ë‹¤'],
                    'moderate': ['ë…¸ë ¥í•˜ë‹¤', 'ì–´ë µë‹¤', 'í˜ë“¤ë‹¤', 'ì¡°ê¸ˆ ì–´ë µë‹¤', 'ë³´í†µì´ë‹¤', 'ê·¸ëŸ­ì €ëŸ­ì´ë‹¤'],
                    'negative': ['ë§¤ìš° ì–´ë µë‹¤', 'ì „í˜€ ëª»í•˜ë‹¤', 'í¬ê¸°í•˜ë‹¤', 'ì‹¤íŒ¨í•˜ë‹¤', 'ë‚˜ì˜ë‹¤', 'ì‹«ë‹¤', 'ì§€ê²¹ë‹¤']
                },
                'sleep_problems': {
                    'positive': ['ì˜ ìë‹¤', 'í¸ì•ˆí•˜ë‹¤', 'ë¬¸ì œì—†ë‹¤', 'ì¢‹ë‹¤', 'ê´œì°®ë‹¤', 'ì¶©ë¶„í•˜ë‹¤'],
                    'moderate': ['ê°€ë” ì–´ë µë‹¤', 'ë³´í†µì´ë‹¤', 'ì¡°ê¸ˆ ì–´ë µë‹¤', 'í˜ë“¤ë‹¤', 'ê·¸ëŸ­ì €ëŸ­ì´ë‹¤'],
                    'negative': ['ë§¤ìš° ì–´ë µë‹¤', 'ì „í˜€ ëª»í•˜ë‹¤', 'ë¶ˆë©´ì¦', 'ì ë“¤ê¸° ì–´ë µë‹¤', 'ìì£¼ ê¹¨ë‹¤', 'í”¼ê³¤í•˜ë‹¤']
                },
                'crying': {
                    'positive': ['ìš¸ì§€ ì•Šë‹¤', 'í‰ì†Œì™€ ê°™ë‹¤', 'ë¬¸ì œì—†ë‹¤', 'ê´œì°®ë‹¤', 'ì•ˆì •ì ì´ë‹¤'],
                    'moderate': ['ê°€ë” ìš¸ë‹¤', 'ì¡°ê¸ˆ ë” ìš¸ë‹¤', 'ë³´í†µì´ë‹¤', 'ê·¸ëŸ­ì €ëŸ­ì´ë‹¤'],
                    'negative': ['ìì£¼ ìš¸ë‹¤', 'í•­ìƒ ìš¸ë‹¤', 'ìš¸ê³  ì‹¶ë‹¤', 'ëˆˆë¬¼ì´ ë‚˜ë‹¤', 'ìŠ¬í”„ë‹¤']
                },
                'fatigue': {
                    'positive': ['í”¼ê³¤í•˜ì§€ ì•Šë‹¤', 'í™œë ¥ìˆë‹¤', 'ì—ë„ˆì§€ê°€ ìˆë‹¤', 'ê´œì°®ë‹¤', 'ê±´ê°•í•˜ë‹¤'],
                    'moderate': ['ê°€ë” í”¼ê³¤í•˜ë‹¤', 'ë³´í†µì´ë‹¤', 'ì¡°ê¸ˆ í”¼ê³¤í•˜ë‹¤', 'ê·¸ëŸ­ì €ëŸ­ì´ë‹¤'],
                    'negative': ['í•­ìƒ í”¼ê³¤í•˜ë‹¤', 'ë§¤ìš° í”¼ê³¤í•˜ë‹¤', 'ê¸°ë ¥ì´ ì—†ë‹¤', 'ë¬´ê¸°ë ¥í•˜ë‹¤', 'ì§€ì¹˜ë‹¤']
                },
                'friendship': {
                    'positive': ['ì¹œêµ¬ê°€ ë§ë‹¤', 'ì‚¬ëŒë“¤ê³¼ ì˜ ì§€ë‚´ë‹¤', 'ì¸ê¸°ê°€ ìˆë‹¤', 'ì¢‹ë‹¤', 'í¸í•˜ë‹¤'],
                    'moderate': ['ì¹œêµ¬ê°€ ì¡°ê¸ˆ ìˆë‹¤', 'ë³´í†µì´ë‹¤', 'ì ë‹¹í•˜ë‹¤', 'ê·¸ëŸ­ì €ëŸ­ì´ë‹¤'],
                    'negative': ['ì¹œêµ¬ê°€ ì—†ë‹¤', 'ì‚¬ëŒë“¤ê³¼ ì–´ë µë‹¤', 'ì™¸ë¡­ë‹¤', 'ì†Œì™¸ê°', 'ì–´ìƒ‰í•˜ë‹¤']
                }
            },
            'rcmas': {
                'anxiety': {
                    'positive': ['ê±±ì •í•˜ì§€ ì•Šë‹¤', 'í‰ì˜¨í•˜ë‹¤', 'ì•ˆì •ì ì´ë‹¤', 'ê´œì°®ë‹¤'],
                    'negative': ['ê±±ì •ì´ ë§ë‹¤', 'ë¶ˆì•ˆí•˜ë‹¤', 'ì´ˆì¡°í•˜ë‹¤', 'ê¸´ì¥ë˜ë‹¤', 'ë‘ë µë‹¤']
                },
                'anger': {
                    'positive': ['í™”ë¥¼ ë‚´ì§€ ì•Šë‹¤', 'ì°¨ë¶„í•˜ë‹¤', 'ì¹¨ì°©í•˜ë‹¤', 'í‰ì˜¨í•˜ë‹¤'],
                    'negative': ['í™”ê°€ ë§ë‹¤', 'ì§œì¦ì´ ë‚˜ë‹¤', 'ì„±ì§ˆì´ ê¸‰í•˜ë‹¤', 'í™”ë¥¼ ì˜ ë‚´ë‹¤']
                },
                'physical_symptoms': {
                    'positive': ['ê±´ê°•í•˜ë‹¤', 'ë¬¸ì œì—†ë‹¤', 'ê´œì°®ë‹¤', 'í¸ì•ˆí•˜ë‹¤'],
                    'negative': ['ì†ì´ ë©”ìŠ¥ê±°ë¦¬ë‹¤', 'ìˆ¨ì´ ì°¨ë‹¤', 'ê°€ìŠ´ì´ ë‹µë‹µí•˜ë‹¤', 'ëª¸ì´ ì•„í”„ë‹¤']
                }
            },
            'bdi': {
                'sleep_pattern': {
                    'positive': ['ì˜ ìë‹¤', 'í¸ì•ˆí•˜ë‹¤', 'ë¬¸ì œì—†ë‹¤', 'ê´œì°®ë‹¤'],
                    'moderate': ['ì¡°ê¸ˆ ì–´ë µë‹¤', 'ê°€ë” ì–´ë µë‹¤', 'ë³´í†µì´ë‹¤'],
                    'negative': ['ë§¤ìš° ì–´ë µë‹¤', 'ì „í˜€ ëª»í•˜ë‹¤', 'ë¶ˆë©´ì¦', 'ìì£¼ ê¹¨ë‹¤']
                },
                'weight_change': {
                    'positive': ['ë³€í™”ì—†ë‹¤', 'ì•ˆì •ì ì´ë‹¤', 'ê´œì°®ë‹¤'],
                    'moderate': ['ì¡°ê¸ˆ ì¤„ë‹¤', 'ì¡°ê¸ˆ ëŠ˜ë‹¤', 'ë³´í†µì´ë‹¤'],
                    'negative': ['ë§ì´ ì¤„ë‹¤', 'ë§ì´ ëŠ˜ë‹¤', 'ê¸‰ê²©í•œ ë³€í™”']
                },
                'appearance': {
                    'positive': ['ê´œì°®ë‹¤', 'ë§Œì¡±í•˜ë‹¤', 'ì¢‹ë‹¤', 'ë¬¸ì œì—†ë‹¤'],
                    'moderate': ['ë³´í†µì´ë‹¤', 'ê·¸ëŸ­ì €ëŸ­ì´ë‹¤'],
                    'negative': ['ë¶ˆë§Œì¡±í•˜ë‹¤', 'ì‹«ë‹¤', 'ìì‹ ì—†ë‹¤', 'ë¶€ë„ëŸ½ë‹¤']
                }
            }
        }
        
        # í†µê³„
        self.stats = {
            'total_processed': 0,
            'successful': 0,
            'failed': 0,
            'start_time': None
        }
    
    def _extract_korean_stems(self, text):
        """í•œêµ­ì–´ í…ìŠ¤íŠ¸ì—ì„œ ì–´ê°„ ì¶”ì¶œ (kiwipiepy ì‚¬ìš©)"""
        if not text:
            return []
        
        try:
            # kiwipiepyë¥¼ ì‚¬ìš©í•œ í˜•íƒœì†Œ ë¶„ì„
            morphs = self.kiwi.analyze(text)
            
            # ì–´ê°„ë§Œ ì¶”ì¶œ (ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬, ë¶€ì‚¬)
            stems = []
            for token in morphs[0][0]:  # ì²« ë²ˆì§¸ ë¬¸ì¥ì˜ ì²« ë²ˆì§¸ ë¶„ì„ ê²°ê³¼
                morph, pos, _, _ = token
                # ì˜ë¯¸ìˆëŠ” í’ˆì‚¬ë§Œ ì¶”ì¶œ
                if pos in ['NNG', 'NNP', 'NNB', 'VV', 'VA', 'VX', 'VCP', 'VCN', 'MAG', 'MAJ']:
                    stems.append(morph)
            
            return stems
            
        except Exception as e:
            print(f"kiwipiepy ë¶„ì„ ì˜¤ë¥˜: {e}")
            # í´ë°±: ê°„ë‹¨í•œ ì–´ê°„ ì¶”ì¶œ
            stems = []
            words = text.split()
            
            for word in words:
                # ê¸°ë³¸ì ì¸ ì–´ë¯¸ ì œê±°
                if word.endswith('ìš”') or word.endswith('ì–´ìš”') or word.endswith('ì•„ìš”'):
                    word = word[:-1] if word.endswith('ìš”') else word[:-2]
                elif word.endswith('ë‹¤') or word.endswith('ì–´') or word.endswith('ì•„'):
                    word = word[:-1]
                elif word.endswith('ê³ ') or word.endswith('ëŠ”') or word.endswith('ì„') or word.endswith('ë¥¼'):
                    word = word[:-1]
                
                if word:
                    stems.append(word)
            
            return stems
    
    def _normalize_korean_text(self, text):
        """í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì •ê·œí™” (ì–´ê°„ ê¸°ë°˜)"""
        if not text:
            return ""
        
        # ì–´ê°„ ì¶”ì¶œ
        stems = self._extract_korean_stems(text)
        
        # ì–´ê°„ë“¤ì„ ê³µë°±ìœ¼ë¡œ ì—°ê²°
        normalized = " ".join(stems)
        
        return normalized
    
    def _create_evaluation_prompt(self, user_response, category, subcategory):
        """Geminiìš© í‰ê°€ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        category_names = {
            'cdi': 'CDI (ì•„ë™ ìš°ìš¸ ì²™ë„)',
            'rcmas': 'RCMAS (ì•„ë™ ë¶ˆì•ˆ ì²™ë„)', 
            'bdi': 'BDI (ë²¡ ìš°ìš¸ ì²™ë„)'
        }
        
        category_name = category_names.get(category, category)
        
        # í‰ê°€ ê¸°ì¤€ í…œí”Œë¦¿ ê°€ì ¸ì˜¤ê¸°
        templates = self.evaluation_templates[category][subcategory]
        
        # í‰ê°€ ê¸°ì¤€ í…ìŠ¤íŠ¸ ìƒì„±
        criteria_text = ""
        for level, keywords in templates.items():
            level_name = {
                'positive': 'ê¸ì •ì /ì •ìƒ',
                'moderate': 'ë³´í†µ/ì¤‘ê°„',
                'negative': 'ë¶€ì •ì /ë¬¸ì œ'
            }.get(level, level)
            criteria_text += f"- {level_name}: {', '.join(keywords)}\n"
        
        # ì‚¬ìš©ì ë‹µë³€ì˜ ì–´ê°„ ì¶”ì¶œ
        user_stems = self._extract_korean_stems(user_response)
        user_normalized = " ".join(user_stems) if user_stems else user_response
        
        prompt = f"""
ë‹¤ìŒì€ {category_name}ì˜ '{subcategory}' í•­ëª©ì— ëŒ€í•œ í‰ê°€ì…ë‹ˆë‹¤.

í‰ê°€ ê¸°ì¤€:
{criteria_text}

ì‚¬ìš©ì ë‹µë³€: "{user_response}"
ì–´ê°„ ë¶„ì„: "{user_normalized}"

ìœ„ ë‹µë³€ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì ìˆ˜ë¥¼ ë§¤ê²¨ì£¼ì„¸ìš”:
- 0ì : ì •ìƒ/ê¸ì •ì  ìƒíƒœ
- 1ì : ë³´í†µ/ì¤‘ê°„ ìƒíƒœ  
- 2ì : ë¬¸ì œ/ë¶€ì •ì  ìƒíƒœ (CDI, BDIì˜ ê²½ìš°)
- 1ì : ë¬¸ì œ/ë¶€ì •ì  ìƒíƒœ (RCMASì˜ ê²½ìš°)

í•œêµ­ì–´ì˜ ì–´ê°„ê³¼ ì–´ë¯¸ë¥¼ ê³ ë ¤í•˜ì—¬ ì˜ë¯¸ë¥¼ ì •í™•íˆ íŒŒì•…í•´ì£¼ì„¸ìš”.
ë‹µë³€ì€ ë°˜ë“œì‹œ ìˆ«ìë§Œ ì¶œë ¥í•´ì£¼ì„¸ìš” (0, 1, ë˜ëŠ” 2).
"""
        return prompt
    
    def _extract_score_from_response(self, response_text, category):
        """ì‘ë‹µì—ì„œ ì ìˆ˜ ì¶”ì¶œ"""
        import re
        numbers = re.findall(r'\d+', response_text)
        
        if not numbers:
            return 1  # ê¸°ë³¸ê°’
        
        score = int(numbers[0])
        
        # ì ìˆ˜ ë²”ìœ„ ê²€ì¦
        if category == 'rcmas':
            # RCMASëŠ” 0 ë˜ëŠ” 1
            return min(score, 1)
        else:
            # CDI, BDIëŠ” 0, 1, 2
            return min(score, 2)
    
    def calculate_similarity_score(self, user_response, category, subcategory):
        """Geminië¥¼ ì‚¬ìš©í•œ ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚° (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
        max_retries = 3
        base_delay = 2  # ê¸°ë³¸ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
        
        for attempt in range(max_retries):
            try:
                prompt = self._create_evaluation_prompt(user_response, category, subcategory)
                
                # Gemini API í˜¸ì¶œ
                response = self.model.generate_content(prompt)
                
                if response and response.text:
                    score = self._extract_score_from_response(response.text, category)
                    return score
                else:
                    print(f"Gemini ì‘ë‹µì´ ë¹„ì–´ìˆìŒ: {user_response}")
                    return 1  # ê¸°ë³¸ê°’
                    
            except Exception as e:
                error_msg = str(e)
                print(f"Gemini API ì˜¤ë¥˜ (ì‹œë„ {attempt+1}/{max_retries}): {error_msg}")
                
                # í• ë‹¹ëŸ‰ ì´ˆê³¼ ì˜¤ë¥˜ì¸ ê²½ìš° ë” ê¸´ ëŒ€ê¸°
                if "429" in error_msg or "quota" in error_msg.lower():
                    delay = base_delay * (2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
                    print(f"í• ë‹¹ëŸ‰ ì´ˆê³¼ë¡œ {delay}ì´ˆ ëŒ€ê¸° ì¤‘...")
                    time.sleep(delay)
                elif attempt < max_retries - 1:
                    # ë‹¤ë¥¸ ì˜¤ë¥˜ì¸ ê²½ìš° ì§§ì€ ëŒ€ê¸°
                    time.sleep(1)
                else:
                    # ë§ˆì§€ë§‰ ì‹œë„ì—ì„œë„ ì‹¤íŒ¨í•˜ë©´ ê¸°ë³¸ê°’ ë°˜í™˜
                    print(f"ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼, ê¸°ë³¸ê°’ ë°˜í™˜")
                    return 1
        
        return 1  # ê¸°ë³¸ê°’
    
    def preprocess_training_data(self, input_file, output_file, batch_size=10):
        """í›ˆë ¨ ë°ì´í„° ì „ì²˜ë¦¬ (ì ìˆ˜í™”) - ë°°ì¹˜ ì²˜ë¦¬ + í”„ë¡œê·¸ë˜ìŠ¤ ë°”"""
        print(f"ğŸš€ í›ˆë ¨ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘ (ë°°ì¹˜ í¬ê¸°: {batch_size})")
        print(f"ì…ë ¥ íŒŒì¼: {input_file}")
        print(f"ì¶œë ¥ íŒŒì¼: {output_file}")
        print(f"ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*80)
        
        # í›ˆë ¨ ë°ì´í„° ë¡œë“œ
        print("ğŸ“‚ í›ˆë ¨ ë°ì´í„° ë¡œë”© ì¤‘...")
        with open(input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        self.stats['start_time'] = time.time()
        self.stats['total_processed'] = len(data)
        
        # ì „ì²´ ëŒ€í™” ìˆ˜ ê³„ì‚°
        total_conversations = len(data)
        total_batches = (total_conversations + batch_size - 1) // batch_size
        
        print(f"ğŸ“Š ì´ {total_conversations}ê°œ ëŒ€í™”, {total_batches}ê°œ ë°°ì¹˜ë¡œ ì²˜ë¦¬")
        print("="*80)
        
        # ì „ì²´ ì§„í–‰ë¥ ì„ ìœ„í•œ í”„ë¡œê·¸ë˜ìŠ¤ ë°”
        with tqdm(total=total_conversations, desc="ì „ì²´ ì§„í–‰ë¥ ", unit="ëŒ€í™”", 
                 bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]") as pbar:
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min(start_idx + batch_size, total_conversations)
                batch_data = data[start_idx:end_idx]
                
                # ë°°ì¹˜ë³„ í”„ë¡œê·¸ë˜ìŠ¤ ë°”
                batch_desc = f"ë°°ì¹˜ {batch_idx+1}/{total_batches}"
                with tqdm(batch_data, desc=batch_desc, unit="ëŒ€í™”", 
                         leave=False, bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}]") as batch_pbar:
                    
                    for i, conversation in enumerate(batch_data):
                        global_idx = start_idx + i
                        
                        # í˜„ì¬ ì§„ë‹¨ ì¹´í…Œê³ ë¦¬ ì¶”ì 
                        current_category = None
                        current_subcategory = None
                        user_responses_processed = 0
                        
                        # ê° ë°œí™” ì²˜ë¦¬
                        for j, turn in enumerate(conversation):
                            if turn['speaker'] == 'system':
                                # ì‹œìŠ¤í…œ ë°œí™”ì—ì„œ ë©”íƒ€ë°ì´í„° í™•ì¸
                                metadata = turn.get('metadata', {})
                                if 'category' in metadata and 'subcategory' in metadata:
                                    current_category = metadata['category']
                                    current_subcategory = metadata['subcategory']
                            
                            elif turn['speaker'] == 'user' and current_category and current_subcategory:
                                user_response = turn['utterance']
                                
                                # ì ìˆ˜ ê³„ì‚°
                                score = self.calculate_similarity_score(user_response, current_category, current_subcategory)
                                
                                # ì ìˆ˜ë¥¼ ë°œí™”ì— ì¶”ê°€
                                turn['score'] = score
                                user_responses_processed += 1
                                self.stats['successful'] += 1
                                
                                # API í˜¸ì¶œ ê°„ê²© (í• ë‹¹ëŸ‰ ì œí•œ ë°©ì§€)
                                time.sleep(2)  # 2ì´ˆ ëŒ€ê¸°ë¡œ í• ë‹¹ëŸ‰ ë¬¸ì œ ë°©ì§€
                        
                        # ë°°ì¹˜ í”„ë¡œê·¸ë˜ìŠ¤ ë°” ì—…ë°ì´íŠ¸
                        batch_pbar.set_postfix({
                            'ì¹´í…Œê³ ë¦¬': f"{current_category}-{current_subcategory}" if current_category else "N/A",
                            'ì²˜ë¦¬ëœë‹µë³€': user_responses_processed,
                            'ì´ì„±ê³µ': self.stats['successful']
                        })
                        batch_pbar.update(1)
                        
                        # ì „ì²´ í”„ë¡œê·¸ë˜ìŠ¤ ë°” ì—…ë°ì´íŠ¸
                        pbar.set_postfix({
                            'ë°°ì¹˜': f"{batch_idx+1}/{total_batches}",
                            'ì„±ê³µ': self.stats['successful'],
                            'ì‹¤íŒ¨': self.stats['failed']
                        })
                        pbar.update(1)
                
                # ë°°ì¹˜ ì™„ë£Œ í›„ ì¤‘ê°„ ì €ì¥
                tqdm.write(f"ğŸ’¾ ë°°ì¹˜ {batch_idx+1} ì™„ë£Œ, ì¤‘ê°„ ì €ì¥ ì¤‘...")
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                
                # ë°°ì¹˜ë³„ í†µê³„
                elapsed = time.time() - self.stats['start_time']
                progress = (batch_idx + 1) / total_batches * 100
                avg_time = elapsed / (batch_idx + 1)
                remaining = (total_batches - (batch_idx + 1)) * avg_time
                
                tqdm.write(f"ğŸ“Š ë°°ì¹˜ {batch_idx+1} ì™„ë£Œ - ì§„í–‰ë¥ : {progress:.1f}%")
                tqdm.write(f"â±ï¸  ê²½ê³¼: {elapsed/60:.1f}ë¶„, ì˜ˆìƒ ë‚¨ì€: {remaining/60:.1f}ë¶„")
                tqdm.write(f"âœ… ì„±ê³µ: {self.stats['successful']}ê°œ, ì‹¤íŒ¨: {self.stats['failed']}ê°œ")
                tqdm.write("-" * 60)
        
        # ìµœì¢… í†µê³„ ì¶œë ¥
        total_time = time.time() - self.stats['start_time']
        print("\n" + "="*80)
        print("ğŸ‰ ì „ì²˜ë¦¬ ì™„ë£Œ!")
        print("="*80)
        print(f"ì™„ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ì´ ì†Œìš” ì‹œê°„: {total_time/60:.1f}ë¶„")
        print(f"ì²˜ë¦¬ëœ ëŒ€í™”: {len(data)}ê°œ")
        print(f"ì„±ê³µí•œ ì ìˆ˜í™”: {self.stats['successful']}ê°œ")
        print(f"ì‹¤íŒ¨í•œ ì ìˆ˜í™”: {self.stats['failed']}ê°œ")
        print(f"í‰ê·  ì²˜ë¦¬ ì‹œê°„: {total_time/len(data):.2f}ì´ˆ/ëŒ€í™”")
        print(f"ë°°ì¹˜ í¬ê¸°: {batch_size}")
        print(f"ì²˜ë¦¬ ì†ë„: {self.stats['successful']/(total_time/60):.1f} ë‹µë³€/ë¶„")
        print("="*80)

def main():
    preprocessor = TrainingDataPreprocessor()
    
    # íŒŒì¼ ê²½ë¡œ
    input_file = 'training_ds/training_dataset.json'
    output_file = 'training_ds/training_dataset_scored.json'
    
    # ì „ì²˜ë¦¬ ì‹¤í–‰
    preprocessor.preprocess_training_data(input_file, output_file)

if __name__ == '__main__':
    main()
