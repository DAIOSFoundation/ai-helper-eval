# -*- coding: utf-8 -*-

import os
import sys
import torch
import gc
import time
from datetime import datetime

# MPS ê°€ì†ì„ ìœ„í•œ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'

from modules.entities import EntityTracker
from modules.bow import BoW_encoder
from modules.rnn_gru_net import RNN_GRU_net
from modules.embed import UtteranceEmbed
from modules.actions import ActionTracker
from modules.similarity_scorer import SimilarityScorer
from modules.data_utils import Data
import modules.util as util
import numpy as np

class Trainer:
    def __init__(self):
        print("í›ˆë ¨ ë°ì´í„° ë¡œë”© ì¤‘...")
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.et = EntityTracker()
        self.bow_enc = BoW_encoder()
        self.emb = UtteranceEmbed()
        self.at = ActionTracker(self.et)
        self.similarity_scorer = SimilarityScorer()
        
        # ë°ì´í„° ë¡œë“œ
        self.dataset, self.dialog_indices = Data(self.et, self.at).trainset
        
        if not self.dataset:
            print("í›ˆë ¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            sys.exit(1)
        
        print(f"ë¡œë“œëœ ëŒ€í™” ìˆ˜: {len(self.dialog_indices)}")
        print(f"ì´ í„´ ìˆ˜: {len(self.dataset)}")
        
        # í›ˆë ¨/ê²€ì¦ ë°ì´í„° ë¶„í• 
        self._split_data()
        
        # ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”
        obs_size = self.emb.dim + self.bow_enc.vocab_size + self.et.num_features
        self.action_templates = self.at.get_action_templates()
        action_size = self.at.action_size
        nb_hidden = 128
        
        print(f"ê´€ì°° í¬ê¸°: {obs_size}")
        print(f"ì•¡ì…˜ í¬ê¸°: {action_size}")
        
        self.net = RNN_GRU_net(obs_size=obs_size,
                               action_size=action_size,
                               nb_hidden=nb_hidden)
    
    def _split_data(self):
        """í›ˆë ¨/ê²€ì¦ ë°ì´í„° ë¶„í• """
        total_dialogs = len(self.dialog_indices)
        train_ratio = 0.8
        
        train_size = int(total_dialogs * train_ratio)
        
        # ëœë¤ ì…”í”Œ
        indices = list(range(total_dialogs))
        np.random.shuffle(indices)
        
        self.dialog_indices_tr = [self.dialog_indices[i] for i in indices[:train_size]]
        self.dialog_indices_dev = [self.dialog_indices[i] for i in indices[train_size:]]
        
        print(f"í›ˆë ¨ ëŒ€í™” ìˆ˜: {len(self.dialog_indices_tr)}")
        print(f"ê²€ì¦ ëŒ€í™” ìˆ˜: {len(self.dialog_indices_dev)}")
    
    def train(self):
        print('\nğŸš€ í›ˆë ¨ ì‹œì‘')
        print(f"ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*60}")
        
        epochs = 20
        start_time = time.time()
        
        # Early stopping ì„¤ì •
        patience = 5  # 5 ì—í¬í¬ ë™ì•ˆ ê°œì„ ì´ ì—†ìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œ
        min_delta = 0.001  # ìµœì†Œ ê°œì„  ì„ê³„ê°’
        best_loss = float('inf')
        patience_counter = 0
        best_model_state = None
        
        # í›ˆë ¨ íˆìŠ¤í† ë¦¬ ì €ì¥
        training_history = {
            'epoch': [],
            'train_loss': [],
            'response_accuracy': [],
            'dialogue_accuracy': [],
            'epoch_time': []
        }
        
        for epoch in range(epochs):
            epoch_start_time = time.time()
            
            print(f"\n{'='*60}")
            print(f"ğŸ”„ ì—í¬í¬ {epoch+1}/{epochs} ì‹œì‘")
            print(f"ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%H:%M:%S')}")
            print(f"{'='*60}")
            
            # í›ˆë ¨
            train_loss = self._train_epoch(epoch+1, epochs)
            
            # ê²€ì¦
            per_response_accuracy, per_dialogue_accuracy = self.evaluate(epoch+1, epochs)
            
            # ì—í¬í¬ ì†Œìš” ì‹œê°„ ê³„ì‚°
            epoch_time = time.time() - epoch_start_time
            total_time = time.time() - start_time
            
            # ê²°ê³¼ ì¶œë ¥
            print(f"\nğŸ“Š ì—í¬í¬ {epoch+1}/{epochs} ê²°ê³¼:")
            print(f"   í›ˆë ¨ ì†ì‹¤: {train_loss:.4f}")
            print(f"   ì‘ë‹µ ì •í™•ë„: {per_response_accuracy:.4f} ({per_response_accuracy*100:.1f}%)")
            print(f"   ëŒ€í™” ì •í™•ë„: {per_dialogue_accuracy:.4f} ({per_dialogue_accuracy*100:.1f}%)")
            print(f"   ì—í¬í¬ ì†Œìš” ì‹œê°„: {epoch_time:.1f}ì´ˆ")
            print(f"   ì´ ì†Œìš” ì‹œê°„: {total_time/60:.1f}ë¶„")
            
            # Early stopping ì²´í¬
            if train_loss < best_loss - min_delta:
                best_loss = train_loss
                patience_counter = 0
                # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
                best_model_state = self.net.state_dict().copy()
                print(f"   ğŸ¯ ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! ì†ì‹¤: {best_loss:.4f}")
            else:
                patience_counter += 1
                print(f"   â³ ê°œì„  ì—†ìŒ ({patience_counter}/{patience})")
            
            # íˆìŠ¤í† ë¦¬ ì €ì¥
            training_history['epoch'].append(epoch+1)
            training_history['train_loss'].append(train_loss)
            training_history['response_accuracy'].append(per_response_accuracy)
            training_history['dialogue_accuracy'].append(per_dialogue_accuracy)
            training_history['epoch_time'].append(epoch_time)
            
            # ì§„í–‰ë¥  ë° ì˜ˆìƒ ì™„ë£Œ ì‹œê°„
            progress = (epoch+1) / epochs * 100
            if epoch > 0:
                avg_epoch_time = sum(training_history['epoch_time']) / len(training_history['epoch_time'])
                remaining_epochs = epochs - (epoch + 1)
                estimated_remaining_time = remaining_epochs * avg_epoch_time
                print(f"   ì „ì²´ ì§„í–‰ë¥ : {progress:.1f}%")
                print(f"   ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {estimated_remaining_time/60:.1f}ë¶„")
            
            # Early stopping ì²´í¬
            if patience_counter >= patience:
                print(f"\nğŸ›‘ Early Stopping! {patience} ì—í¬í¬ ë™ì•ˆ ê°œì„ ì´ ì—†ì—ˆìŠµë‹ˆë‹¤.")
                print(f"   ìµœê³  ì†ì‹¤: {best_loss:.4f} (ì—í¬í¬ {training_history['epoch'][training_history['train_loss'].index(best_loss)]})")
                break
            
            print(f"{'='*60}")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë³µì› ë° ì €ì¥
        if best_model_state is not None:
            self.net.load_state_dict(best_model_state)
            print(f"\nğŸ’¾ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë³µì› ì™„ë£Œ (ì†ì‹¤: {best_loss:.4f})")
        
        # ëª¨ë¸ ì €ì¥
        self.net.save()
        total_training_time = time.time() - start_time
        
        print(f"\nğŸ‰ í›ˆë ¨ ì™„ë£Œ!")
        print(f"ì™„ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ì´ í›ˆë ¨ ì‹œê°„: {total_training_time/60:.1f}ë¶„")
        print(f"ì‹¤ì œ í›ˆë ¨ ì—í¬í¬: {len(training_history['epoch'])}/{epochs}")
        
        # ìµœì¢… ê²°ê³¼ ìš”ì•½
        print(f"\nğŸ“ˆ ìµœì¢… í›ˆë ¨ ê²°ê³¼:")
        print(f"   ìµœì¢… í›ˆë ¨ ì†ì‹¤: {training_history['train_loss'][-1]:.4f}")
        print(f"   ìµœì¢… ì‘ë‹µ ì •í™•ë„: {training_history['response_accuracy'][-1]:.4f} ({training_history['response_accuracy'][-1]*100:.1f}%)")
        print(f"   ìµœì¢… ëŒ€í™” ì •í™•ë„: {training_history['dialogue_accuracy'][-1]:.4f} ({training_history['dialogue_accuracy'][-1]*100:.1f}%)")
        print(f"   í‰ê·  ì—í¬í¬ ì‹œê°„: {sum(training_history['epoch_time'])/len(training_history['epoch_time']):.1f}ì´ˆ")
    
    def _train_epoch(self, current_epoch, total_epochs):
        """í•œ ì—í¬í¬ í›ˆë ¨ (ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© í‘œì‹œ)"""
        total_loss = 0.0
        num_examples = 0
        
        print(f"  ğŸ”„ í›ˆë ¨ ëŒ€í™” ìˆ˜: {len(self.dialog_indices_tr)}")
        
        for i, dialog_idx in enumerate(self.dialog_indices_tr):
            start, end = dialog_idx['start'], dialog_idx['end']
            dialog = self.dataset[start:end]
            
            # ëŒ€í™”ë³„ í›ˆë ¨
            loss = self._train_dialog(dialog)
            total_loss += loss * len(dialog)
            num_examples += len(dialog)
            
            # ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© í‘œì‹œ (10ê°œë§ˆë‹¤, ì¶œë ¥ ì˜¤ë²„í—¤ë“œ ê°ì†Œ)
            if (i + 1) % 10 == 0:
                progress = (i + 1) / len(self.dialog_indices_tr) * 100
                avg_loss = total_loss / num_examples if num_examples > 0 else 0.0
                print(f"    ì§„í–‰: {i+1}/{len(self.dialog_indices_tr)} ëŒ€í™” ì™„ë£Œ ({progress:.1f}%) - í‰ê·  ì†ì‹¤: {avg_loss:.4f}")
                
                # MPS ë©”ëª¨ë¦¬ ê´€ë¦¬ (ë¹ˆë„ ê°ì†Œ)
                if torch.backends.mps.is_available():
                    torch.mps.empty_cache()
                    torch.mps.synchronize()
                gc.collect()
        
        avg_loss = total_loss / num_examples if num_examples > 0 else 0.0
        print(f"  âœ… ì—í¬í¬ {current_epoch} í›ˆë ¨ ì™„ë£Œ - í‰ê·  ì†ì‹¤: {avg_loss:.4f}")
        
        return avg_loss
    
    def _train_dialog(self, dialog):
        """ëŒ€í™”ë³„ í›ˆë ¨ (ì£¼ê´€ì‹ í‰ê°€ ê¸°ë°˜, ì†ë„ ìµœì í™”)"""
        # ì—”í‹°í‹° íŠ¸ë˜ì»¤ì™€ ì•¡ì…˜ íŠ¸ë˜ì»¤ ì´ˆê¸°í™”
        et = EntityTracker()
        at = ActionTracker(et)
        
        # ë„¤íŠ¸ì›Œí¬ ìƒíƒœ ì´ˆê¸°í™”
        self.net.reset_state()
        
        total_loss = 0.0
        valid_examples = 0
        
        # ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë°ì´í„° ìˆ˜ì§‘
        batch_features = []
        batch_targets = []
        batch_masks = []
        
        for turn_data in dialog:
            if isinstance(turn_data, tuple):
                # ê¸°ì¡´ í˜•ì‹: (user_utterance, target_action)
                user_utterance, target_action = turn_data
                precomputed_score = None
            else:
                # ìƒˆë¡œìš´ í˜•ì‹: ë”•ì…”ë„ˆë¦¬ (ì ìˆ˜í™”ëœ ë°ì´í„°)
                user_utterance = turn_data.get('utterance')
                target_action = turn_data.get('action')
                precomputed_score = turn_data.get('score')
            
            if user_utterance is None or target_action is None:
                continue
            
            try:
                # í˜„ì¬ ì§ˆë¬¸ì— ëŒ€í•œ í‰ê°€ í•­ëª© ì„¤ì •
                at.set_current_evaluation(target_action)
                
                # íŠ¹ì„± ì¶”ì¶œ (ì ìˆ˜í™”ëœ ë°ì´í„° ì‚¬ìš©)
                features = self._extract_features(user_utterance, et, precomputed_score)
                
                # ì•¡ì…˜ ë§ˆìŠ¤í¬
                action_mask = at.action_mask
                
                # ë°°ì¹˜ì— ì¶”ê°€
                batch_features.append(features)
                batch_targets.append(target_action)
                batch_masks.append(action_mask)
                
            except Exception as e:
                print(f"íŠ¹ì„± ì¶”ì¶œ ì˜¤ë¥˜ (ê±´ë„ˆëœ€): {e}")
                continue
        
        # ë°°ì¹˜ë³„ í›ˆë ¨ (ë” íš¨ìœ¨ì )
        if batch_features:
            for features, target, mask in zip(batch_features, batch_targets, batch_masks):
                try:
                    loss = self.net.train_step(features, target, mask)
                    total_loss += loss
                    valid_examples += 1
                except Exception as e:
                    print(f"í›ˆë ¨ ìŠ¤í… ì˜¤ë¥˜ (ê±´ë„ˆëœ€): {e}")
                    continue
        
        return total_loss / valid_examples if valid_examples > 0 else 0.0
    
    def _extract_features(self, utterance, et, precomputed_score=None):
        """ë°œí™”ì—ì„œ íŠ¹ì„± ì¶”ì¶œ (ì ìˆ˜í™”ëœ ë°ì´í„° ì§€ì›)"""
        # ì—”í‹°í‹° ì¶”ì¶œ (ì ìˆ˜í™”ëœ ë°ì´í„° ìš°ì„  ì‚¬ìš©)
        u_ent, u_entities = et.extract_entities(
            utterance, 
            is_test=False, 
            similarity_scorer=self.similarity_scorer,
            precomputed_score=precomputed_score
        )
        u_ent_features = et.context_features()
        
        # ì„ë² ë”©
        u_emb = self.emb.encode(utterance)
        u_bow = self.bow_enc.encode(utterance)
        
        # íŠ¹ì„± ê²°í•©
        features = np.concatenate((u_ent_features, u_emb, u_bow), axis=0)
        
        return features  # numpy ë°°ì—´ë¡œ ë°˜í™˜ (RNN_GRU_netì—ì„œ í…ì„œë¡œ ë³€í™˜)
    
    def evaluate(self, current_epoch, total_epochs):
        """ëª¨ë¸ í‰ê°€ (ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© í‘œì‹œ)"""
        self.net.eval()
        
        dialog_accuracy = 0.0
        correct_dialogue_count = 0
        
        print(f"  ğŸ” ê²€ì¦ ëŒ€í™” ìˆ˜: {len(self.dialog_indices_dev)}")
        
        for i, dialog_idx in enumerate(self.dialog_indices_dev):
            start, end = dialog_idx['start'], dialog_idx['end']
            dialog = self.dataset[start:end]
            
            # ëŒ€í™”ë³„ í‰ê°€
            correct_examples = self._evaluate_dialog(dialog)
            
            # ëŒ€í™” ì™„ì „ ì •í™•ë„
            if correct_examples == len(dialog):
                correct_dialogue_count += 1
            
            # ì‘ë‹µ ì •í™•ë„
            dialog_accuracy += correct_examples / len(dialog)
            
            # ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© í‘œì‹œ (5ê°œë§ˆë‹¤, ì¶œë ¥ ì˜¤ë²„í—¤ë“œ ê°ì†Œ)
            if (i + 1) % 5 == 0:
                progress = (i + 1) / len(self.dialog_indices_dev) * 100
                current_accuracy = dialog_accuracy / (i + 1)
                print(f"    ê²€ì¦ ì§„í–‰: {i+1}/{len(self.dialog_indices_dev)} ëŒ€í™” ì™„ë£Œ ({progress:.1f}%) - í˜„ì¬ ì •í™•ë„: {current_accuracy:.4f}")
        
        num_dev_examples = len(self.dialog_indices_dev)
        per_response_accuracy = dialog_accuracy / num_dev_examples if num_dev_examples > 0 else 0.0
        per_dialogue_accuracy = correct_dialogue_count / num_dev_examples if num_dev_examples > 0 else 0.0
        
        print(f"  âœ… ì—í¬í¬ {current_epoch} ê²€ì¦ ì™„ë£Œ - ì‘ë‹µ ì •í™•ë„: {per_response_accuracy:.4f}, ëŒ€í™” ì •í™•ë„: {per_dialogue_accuracy:.4f}")
        
        return per_response_accuracy, per_dialogue_accuracy
    
    def _evaluate_dialog(self, dialog):
        """ëŒ€í™”ë³„ í‰ê°€"""
        # ì—”í‹°í‹° íŠ¸ë˜ì»¤ì™€ ì•¡ì…˜ íŠ¸ë˜ì»¤ ì´ˆê¸°í™”
        et = EntityTracker()
        at = ActionTracker(et)
        
        # ë„¤íŠ¸ì›Œí¬ ìƒíƒœ ì´ˆê¸°í™”
        self.net.reset_state()
        
        correct_examples = 0
        
        for turn_data in dialog:
            if isinstance(turn_data, tuple):
                # ê¸°ì¡´ í˜•ì‹: (user_utterance, target_action)
                user_utterance, target_action = turn_data
            else:
                # ìƒˆë¡œìš´ í˜•ì‹: ë”•ì…”ë„ˆë¦¬ (ì ìˆ˜í™”ëœ ë°ì´í„°)
                user_utterance = turn_data.get('utterance')
                target_action = turn_data.get('action')
            if user_utterance is None or target_action is None:
                continue
            
            # íŠ¹ì„± ì¶”ì¶œ
            features = self._extract_features(user_utterance, et)
            
            # ì•¡ì…˜ ë§ˆìŠ¤í¬
            action_mask = at.action_mask
            
            # ì˜ˆì¸¡
            prediction = self.net.forward(features, action_mask)
            
            # ì •í™•ë„ ê³„ì‚°
            if prediction == target_action:
                correct_examples += 1
        
        return correct_examples

if __name__ == '__main__':
    # ëœë¤ ì‹œë“œ ì„¤ì •
    util.set_seed(42)
    
    # í›ˆë ¨ ì‹œì‘
    trainer = Trainer()
    trainer.train()
